import json
import asyncio
import os
from openai import AsyncOpenAI
from dotenv import load_dotenv

load_dotenv()

client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

EVALUATION_PROMPT = """You are an expert judge evaluating the performance of an AI travel agent.
I will provide you with:
1. A User Query.
2. A description of a "Previous Best Response" (Orig) and its score (0-10).
3. A "New Agent Response" generated by a new architecture.

Your task is to:
1. Score the New Agent Response (0-10) based on helpfulness, accuracy (implied), and completeness.
2. Compare the New Response to the Previous Best explanation.
3. Decide if the New Response is "Better", "Worse", or "Similar" to the Previous Best.
4. Provide a brief explanation.

Input Data:
User Query: {query}
Previous Best Analysis: {original_explanation} (Previous Score: {original_score})
New Agent Response: {new_response}

Output JSON format:
{{
  "new_score": <number>,
  "comparison": "Better" | "Worse" | "Similar",
  "reason": "<string>"
}}
"""

async def evaluate_case(case):
    # Determine the benchmark score (Orig was the winner or part of a draw)
    # The original result was either ORIG or DRAW. 
    # If DRAW, both scores should be similar. If ORIG, Orig score is the benchmark.
    orig_score = case["original_scores"]["orig"]
    
    prompt = EVALUATION_PROMPT.format(
        query=case["query"],
        original_explanation=case["original_explanation"],
        original_score=orig_score,
        new_response=case.get("new_agent_response", "No response generated.")
    )

    try:
        response = await client.chat.completions.create(
            model="gpt-4o", # Use a strong model for evaluation
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content
        return json.loads(content)
    except Exception as e:
        print(f"Error evaluating case: {e}")
        return {
            "new_score": 0,
            "comparison": "Error",
            "reason": str(e)
        }

async def main():
    print("Reading test_results2.json...")
    with open("test_results2.json", "r", encoding="utf-8") as f:
        data = json.load(f)

    print(f"Evaluating {len(data)} cases...")
    
    # Process in batches to avoid rate limits if necessary, but 50 is small enough for parallel specific limits
    # We'll do semaphores just in case
    sem = asyncio.Semaphore(10)

    async def run_with_sem(case):
        async with sem:
            print(f"Evaluating: {case['query'][:30]}...")
            eval_result = await evaluate_case(case)
            case["evaluation"] = eval_result
            return case

    tasks = [run_with_sem(case) for case in data]
    evaluated_results = await asyncio.gather(*tasks)

    print("Saving evaluated results to test_results2.json...")
    with open("test_results2.json", "w", encoding="utf-8") as f:
        json.dump(evaluated_results, f, indent=2, ensure_ascii=False)
    
    # Calculate summary stats
    better = sum(1 for c in evaluated_results if c["evaluation"]["comparison"] == "Better")
    similar = sum(1 for c in evaluated_results if c["evaluation"]["comparison"] == "Similar")
    worse = sum(1 for c in evaluated_results if c["evaluation"]["comparison"] == "Worse")
    
    print(f"Evaluation Complete.")
    print(f"Better: {better}")
    print(f"Similar: {similar}")
    print(f"Worse: {worse}")

if __name__ == "__main__":
    asyncio.run(main())
